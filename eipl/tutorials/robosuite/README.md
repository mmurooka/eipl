<!-- ******************************** -->
# Installation

Clone the robosuite repository
```bash
$ git clone https://github.com/ARISE-Initiative/robosuite.git
$ cd robosuite
```

Install the requirements with
```bash
$ pip3 install -r requirements.txt
$ pip3 install -r requirements-extra.txt
```



<!-- ******************************** -->
# Dataset
## Step 1: Motion Teaching
Teach object grasping motions to the robot using keyboard teleoperation.
The argument `pos` is the position of the object, and `ep_dir` is the save directory.
We provide a `demonstration dataset` to eliminate the hassle of data collection. For more information, [click here](#demonstration-dataset).

```bash 
$ cd eipl/tutorials/robosuite/simulator
$ ls
2_resave.sh  3_check_data.sh  bin  data  libs  output  README
$ python3 ./bin/1_teaching.py --pos target_position --ep_dir save_dir

# e.g.
$ python3 ./bin/1_teaching.py --pos -0.2 --ep_dir ./data/raw_data/Pos1_1

[robosuite WARNING] No private macro file found! (__init__.py:7)
[robosuite WARNING] It is recommended to use a private macro file (__init__.py:8)
[robosuite WARNING] To setup, run: python /home/ito/Downloads/robosuite/robosuite/scripts/setup_macros.py (__init__.py:9)

Keys      	Command
spacebar  	toggle gripper (open/close)
w-a-s-d   	move arm horizontally in x-y plane
r-f       	move arm vertically
z-x       	rotate arm about x-axis
t-g       	rotate arm about y-axis
c-v       	rotate arm about z-axis
[         	start recording
]         	stop recording (not recommended)

Collecting demonstration data ...
```

The collected data (model.xml and state.npz) are stored in the `ep_dir` folder (e.g. ./data/raw_data/).
```bash
$ ls ./data/raw_data/
Pos1_1  Pos1_2  Pos1_3  Pos1_4  Pos2_1  Pos2_2  Pos2_3  Pos2_4 ...
$ cd ./data/raw_data/Pos1_1
model.xml  state.npz
```


<!-- ******************************** -->
----
## Step 2: Data Re-Saving
The following two processes are performed.
The first is the removal of guide lines. During keyboard teleoperation, green guide lines appear in the image data. By playback of the collected data, the image data without the guide line is saved. The second is downsampling. During tele-operation, the collected data sequence is long (default is 600 step) because the robot is controlled with a high frequency to teach fine movements. Therefore, sensor data is recollected every 5 step to downsample the sequence length to 120 step.

```bash
# Re-save the files one by one.
$ python3 ./bin/2_resave.py --ep_dir ./data/raw_data/Pos1_1/

# Re-save all files in the specified folder (default is './data/raw_data')  at once.
$ bash 2_resave.sh
```



<!-- ******************************** -->
----
## Step 3: Check Saved Data
Check whether the motion generated by playback is correct. Tasks rarely fail due to downsampling. If the playback data fails the task, the following error is displayed `[ERROR] This data set has failed task during playback.`. If the task is successful, the gif animation is saved in the output folder.

```bash
# Re-save the files one by one.
$ python3 ./bin/3_check_playback_data.py ./data/raw_data/Pos1_1/state_resave.npz
$ ls ./output/
Pos1_1_image_joint_ani.gif

# Re-save all files in the specified folder (default is './data/raw_data')  at once.
$ bash 3_check_data.sh
```



<!-- ******************************** -->
----
## Step 4: Generate Dataset
If the playbacked file was stored in `./data/raw_data/`, training/test data will be automatically generated by the following command.

```bash
$ python3 ./bin/4_generate_dataset.py
$ ls ./data/
joint_bounds.npy  pose_bounds.npy  raw_data  test  train
$ ls ./data/train/
images.npy  joints.npy  poses.npy
$ ls ./data/test/
images.npy  joints.npy  poses.npy
```



<!-- ******************************** -->
----
## Step 5: Check Dataset
Check for proper normalization range of joint angles. The visual image of the robot, raw joint angles, and normalized joint angle data are saved as a gif animation in the output folder.

```bash
$ python3 ./bin/5_check_dataset.py
load test data, index number is 0
Joint: shape=(18, 90, 8), min=-2.64, max=3.18
Norm joint: shape=(18, 90, 8), min=0.1, max=0.9

$ ls ./output/
check_dataset_0.gif
```

<!-- ******************************** -->
----
## Demonstration Dataset
Download the demonstration dataset from [this link](https://drive.google.com/file/d/1Y7emK_D5_wJebto1AO_sDnW2RFqlrok_/view?usp=sharing) and extract it into `./data/raw_data/` to eliminate the need for data collection. Note that after downloading the file, you must perform [Step 2](#step-2-data-re-saving) or later.



# Train Model
Move to the `sarnn folder` and start training the model. The trained weights are saved in the log folder. If you want to perform validation without training the model, download the trained weights from [here](https://drive.google.com/file/d/1H4NKKAsNG0gwPemBR1N34s5A94KOH6R1/view?usp=sharing) and save it in the log folder.


```bash
$ cd ../sarnn/
$ python3 ./bin/train.py
[INFO] Set tag = YEAR_DAY_TIME
================================
batch_size : 5
compile : False
device : 0
epoch : 10000
heatmap_size : 0.1
img_loss : 0.1
joint_loss : 1.0
k_dim : 5
log_dir : log/
lr : 0.0001
model : sarnn
n_worker : 8
optimizer : adam
pt_loss : 0.1
rec_dim : 50
stdev : 0.1
tag : YEAR_DAY_TIME
temperature : 0.0001
vmax : 1.0
vmin : 0.0
================================
 96%|███████████████████ | 9551/10000 [1:56:01<05:31,  1.35it/s, train_loss=0.00066, test_loss=0.00111]
```

## Test
Specifying a weight file as the argument of `test.py` will save a gif animation of the predicted image, attention points, and predicted joint angles in the output folder.

```bash
$ python3 ./bin/test.py --filename ./log/YEAR_DAY_TIME/SARNN.pth
$ ls ./output/
$ SARNN_YEAR_DAY_TIME_0.gif
```

## Visualization of internal representation using PCA
Specifying a weight file as the argument of `test_pca_sarnn.py` will save the internal representation of the RNN as a gif animation.

```bash
$ python3 ./bin/test_pca_sarnn.py ./log/YEAR_DAY_TIME/SARNN.pth
$ ls ./output/
$ PCA_SARNN_YEAR_DAY_TIME.gif
```


# Realtime Motion Generation
Move to the simulator folder and run the realtime motion generation program `6_rt_control.py` with the weight file as an argument. This program repeats 10 times the object grasping operation placed at random positions.

```bash
$ cd ../simulator/
$ python3 ./bin/6_rt_control.py ../sarnn/log/YEAR_DAY_TIME/SARNN.pth
[robosuite WARNING] No private macro file found! (macros.py:53)
[robosuite WARNING] It is recommended to use a private macro file (macros.py:54)
[robosuite WARNING] To setup, run: python /home/ito/var/robosuite/robosuite/scripts/setup_macros.py (macros.py:55)
[1/10] Task succeeded!
[2/10] Task succeeded!
```